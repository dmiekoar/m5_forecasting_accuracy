{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting Accuracy \n",
    "\n",
    "Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition\n",
    "\n",
    "How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.\n",
    "\n",
    "The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.\n",
    "\n",
    "In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n",
    "\n",
    "If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.\n",
    "\n",
    "Acknowledgements\n",
    "Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings, gc, sys, json\n",
    "from m5_utils import *\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "calendar = pd.read_csv('input/calendar.csv'); del calendar['weekday']\n",
    "data = pd.read_csv('input/sales_train_validation.csv')\n",
    "submission = pd.read_csv('input/sample_submission.csv')\n",
    "sell_prices = pd.read_csv('input/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(data.shape)\n",
    "data.head().append(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1969, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  wm_yr_wk  wday  month  year       d  event_name_1  \\\n",
       "0     2011-01-29     11101     1      1  2011     d_1           NaN   \n",
       "1     2011-01-30     11101     2      1  2011     d_2           NaN   \n",
       "2     2011-01-31     11101     3      1  2011     d_3           NaN   \n",
       "3     2011-02-01     11101     4      2  2011     d_4           NaN   \n",
       "4     2011-02-02     11101     5      2  2011     d_5           NaN   \n",
       "1964  2016-06-15     11620     5      6  2016  d_1965           NaN   \n",
       "1965  2016-06-16     11620     6      6  2016  d_1966           NaN   \n",
       "1966  2016-06-17     11620     7      6  2016  d_1967           NaN   \n",
       "1967  2016-06-18     11621     1      6  2016  d_1968           NaN   \n",
       "1968  2016-06-19     11621     2      6  2016  d_1969  NBAFinalsEnd   \n",
       "\n",
       "     event_type_1  event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0             NaN           NaN          NaN        0        0        0  \n",
       "1             NaN           NaN          NaN        0        0        0  \n",
       "2             NaN           NaN          NaN        0        0        0  \n",
       "3             NaN           NaN          NaN        1        1        0  \n",
       "4             NaN           NaN          NaN        1        0        1  \n",
       "1964          NaN           NaN          NaN        0        1        1  \n",
       "1965          NaN           NaN          NaN        0        0        0  \n",
       "1966          NaN           NaN          NaN        0        0        0  \n",
       "1967          NaN           NaN          NaN        0        0        0  \n",
       "1968     Sporting  Father's day     Cultural        0        0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(calendar.shape)\n",
    "calendar.head().append(calendar.tail())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(sell_prices.shape)\n",
    "sell_prices.head().append(sell_prices.tail())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(submission.shape)\n",
    "submission.head().append(submission.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sub(submission,generate_dict = None):\n",
    "    \n",
    "    # Identify ids used for validation and evaluation\n",
    "    test_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    val_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    \n",
    "    # Create template for validation and evaluation\n",
    "    test = submission[submission['id'].isin(test_rows)]\n",
    "    val = submission[submission['id'].isin(val_rows)]\n",
    "    \n",
    "    # Identify which forecasting days belong to validation and evaluation\n",
    "    n = 1914\n",
    "    test_days = np.arange(n, n+28, 1)\n",
    "    val_days = np.arange(n+28, n+28+28, 1)\n",
    "    test_columns = ['id']+['d_'+ str(value) for value in test_days]\n",
    "    val_columns = ['id']+['d_'+ str(value) for value in val_days]\n",
    "    \n",
    "    # Creates a dict to later be used as reference when submitting\n",
    "    sub_dict_1 = {}\n",
    "    if generate_dict is not None:\n",
    "        sub_dict_1['test'] = (dict(zip(test_columns[1::],submission.columns[1::])))\n",
    "        sub_dict_1['val'] = (dict(zip(val_columns[1::],submission.columns[1::])))\n",
    "\n",
    "    # Replace columns name\n",
    "    test.columns = test_columns\n",
    "    val.columns = val_columns\n",
    "    \n",
    "\n",
    "    test = pd.melt(test, id_vars= 'id', var_name= 'day', value_name= 'demand')\n",
    "    val = pd.melt(val, id_vars= 'id', var_name= 'day', value_name= 'demand')\n",
    "    \n",
    "    return test, val, sub_dict_1\n",
    "\n",
    "def create_dict(train, calendar):\n",
    "    # 1:'Saturday', 7:'Friday'\n",
    "    \n",
    "    sub_dict1 = {}\n",
    "    # Define columns that will be in the dictionary\n",
    "    cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    # Loop through the train dataframe to get the values for the dictionary\n",
    "    for col in train.columns:\n",
    "        if col in cols:\n",
    "            # Set the column as category and generate a dictionary out of it\n",
    "            train[col] = train[col].astype('category')\n",
    "            tmp_dict = dict(enumerate(train[col].cat.categories))\n",
    "            # Compile the information into a nested dictionary\n",
    "            sub_dict1[col] = tmp_dict\n",
    "\n",
    "    # Define other columns that will be in the dictionary\n",
    "    cols = [ 'wm_yr_wk', 'd','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "    # Loop through the train dataframe to get the values for the dictionary\n",
    "    for col in calendar.columns:\n",
    "        if col in cols:\n",
    "            # Set the column as category and generate a dictionary out of it\n",
    "            calendar[col] = calendar[col].astype('category')\n",
    "            tmp_dict = dict(enumerate(calendar[col].cat.categories))\n",
    "            # Compile the information into a nested dictionary\n",
    "            sub_dict1[col] = tmp_dict\n",
    "    sub_dict1['year'] = {1: 2011, 2: 2012, 3: 2013, 4: 2014, 5: 2015, 6: 2016}\n",
    "    sub_dict1['source'] = {0:'train', 1:'val', 2:'test'}\n",
    "\n",
    "    return train, calendar, sub_dict1\n",
    "\n",
    "def create_inv_dict(source_dict):\n",
    "    \n",
    "    inv_dict = {}\n",
    "\n",
    "    for key, value in source_dict.items():\n",
    "        tmp_dict = {}\n",
    "        for i, item in value.items():\n",
    "            if key != 'year':\n",
    "                tmp_dict[item] = i\n",
    "        inv_dict[key] = tmp_dict\n",
    "    \n",
    "    inv_dict['year'] = {2011:1, 2012:2, 2013:3, 2014:4, 2015:5, 2016:6}\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def preprocess(data, submission, calendar = calendar, generate_dict = None):\n",
    "    \n",
    "    if generate_dict is not None:\n",
    "        data, calendar, dict_m5_1 = create_dict(data, calendar)\n",
    "    else:\n",
    "        dict_m5_1 = {}\n",
    "    \n",
    "    product = data[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n",
    "    train = pd.melt(data, id_vars= data.iloc[:,0:6].columns, var_name= 'day', value_name= 'demand')\n",
    "    train_columns = train.columns\n",
    "    train['source'] = 'train'\n",
    "    \n",
    "    test, val, dict_m5_2 = preprocess_sub(submission, generate_dict)\n",
    "    \n",
    "    # Merge both dictionaries\n",
    "    dict_m5 = dict_m5_1\n",
    "    dict_m5.update(dict_m5_2)\n",
    "        \n",
    "    # Merge products to test and validation\n",
    "    test = test.merge(product, how = 'left', on = 'id')\n",
    "    val['id'] = val['id'].transform(lambda x: x.replace('_evaluation','_validation'))\n",
    "    val = val.merge(product, how = 'left', on = 'id')\n",
    "    val['id'] = val['id'].transform(lambda x: x.replace('_validation','_evaluation'))\n",
    "\n",
    "    test = test[train_columns]\n",
    "    val = val[train_columns]\n",
    "    test['source'] = 'test'\n",
    "    val['source'] = 'val'\n",
    "    \n",
    "    del product, submission, data\n",
    "    \n",
    "    dict_m5_inv = create_inv_dict(dict_m5)\n",
    "    \n",
    "    return train, test, val, calendar, dict_m5, dict_m5_inv\n",
    "    \n",
    "\n",
    "def merge_df(df, calendar, sell_prices = None):\n",
    "    \n",
    "    df = pd.merge(df, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "    del df['d'], df['day']\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    if sell_prices is not None:\n",
    "        df = df.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        df = reduce_mem_usage(df)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train, test, val, calendar, dict_m5, dict_m5_inv = preprocess(data, submission, calendar, generate_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input/dict_m5.json') as json_file:\n",
    "    dict_m5 = json.load(json_file)\n",
    "with open('input/dict_m5_inv.json') as json_file:\n",
    "    dict_m5_inv = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input/dict_m5.json', 'w') as json_file:\n",
    "    json.dump(dict_m5, json_file)\n",
    "    \n",
    "with open('input/dict_m5_inv.json', 'w') as json_file:\n",
    "    json.dump(dict_m5_inv, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = merge_df(train, calendar, sell_prices)\n",
    "print(train.shape); train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = merge_df(test, calendar, sell_prices)\n",
    "print(test.shape); test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val = merge_df(val, calendar, sell_prices)\n",
    "print(val.shape); val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test.to_pickle('input/test_v0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val.to_pickle('input/val_v0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test, val, data, calendar, sell_prices, submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2011 = train.loc[train['year']==2011,:]\n",
    "train_2011.to_pickle('input/train_2011.pkl')\n",
    "del train_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2012 = train.loc[train['year']==2012,:]\n",
    "train_2012.to_pickle('input/train_2012.pkl')\n",
    "del train_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2013 = train.loc[train['year']==2013,:]\n",
    "train_2013.to_pickle('input/train_2013.pkl')\n",
    "del train_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2014 = train.loc[train['year']==2014,:]\n",
    "train_2014.to_pickle('input/train_2014.pkl')\n",
    "del train_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2015 = train.loc[train['year']==2015,:]\n",
    "train_2015.to_pickle('input/train_2015.pkl')\n",
    "del train_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2016 = train.loc[train['year']==2016,:]\n",
    "train_2016.to_pickle('input/train_2016.pkl')\n",
    "del train_2016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
